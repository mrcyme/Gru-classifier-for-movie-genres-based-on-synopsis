{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Predict movie genres",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPKUC7LextEm1IvBA2VzpEA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrcyme/Gru-classifier-for-movie-genres-based-on-synopsis/blob/orphan_branch/Predict_movie_genres.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQ4YHdCliIEa",
        "outputId": "dd874e9b-e1dc-4c11-b7ef-fa9f25c745df"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0eH-iTXgh6Y",
        "outputId": "b6c5dc90-963a-4914-f9f9-2ddccadfc39b"
      },
      "source": [
        "\"\"\"Module does blah blah.\"\"\"\n",
        "\n",
        "from flask import Flask, request, jsonify, Response\n",
        "import io\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "import json\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, GRU\n",
        "from tensorflow.keras.preprocessing import text, sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam, Adagrad,RMSprop\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "app = Flask(__name__)\n",
        "MAX_WORDS = 80000\n",
        "EMB_DIM = 100\n",
        "NUMBER_OF_GENRES = 19\n",
        "MULTILABEL_BINARIZER = MultiLabelBinarizer()\n",
        "BATCH_SIZE = 128\n",
        "N_EPOCHS = 10\n",
        "MAX_VECTOR_LEN = 200\n",
        "DROUPOUT_RATE = 0.5\n",
        "HIDDEN_DIM = 128\n",
        "LEARNING_RATE = 0.002\n",
        "EARLY_STOPPING = EarlyStopping(monitor='val_loss', \n",
        "                               mode='min',\n",
        "                               restore_best_weights=True)\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean text.\"\"\"\n",
        "    text = re.sub(\"\\'\", \"\", text)\n",
        "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
        "    text = text.lower()\n",
        "    stopWords = set(stopwords.words('english'))\n",
        "    text = ' '.join([w for w in text.split() if w not in stopWords])\n",
        "    return text\n",
        "\n",
        "\n",
        "def get_embedded_synopsis(df,tokenizer=None):\n",
        "    \"\"\"Preprocess synopsis.\"\"\"\n",
        "    x_train = df['synopsis'].apply(lambda x: clean_text(x)).to_numpy()\n",
        "    if not tokenizer:\n",
        "      tokenizer = text.Tokenizer(num_words=MAX_WORDS)\n",
        "      tokenizer.fit_on_texts(x_train)\n",
        "      with open('tokenizer.json', 'w', encoding='utf-8') as f:  \n",
        "          f.write(json.dumps(tokenizer.to_json(), ensure_ascii=False))\n",
        "    x_train_seq = tokenizer.texts_to_sequences(x_train)\n",
        "    x_train_pad = sequence.pad_sequences(x_train_seq, maxlen=MAX_VECTOR_LEN)\n",
        "    return x_train_pad\n",
        "\n",
        "\n",
        "def get_one_hot_genres(df):\n",
        "    \"\"\"Return the genres under one hot form.\"\"\"\n",
        "    genres = df['genres'].apply(lambda x: x.split(\" \")).to_numpy()\n",
        "    y_train_one_hot = MULTILABEL_BINARIZER.fit_transform(genres)\n",
        "    y_train_one_hot = np.array([[x / np.sum(row) for x in row] for row in y_train_one_hot])\n",
        "    return y_train_one_hot\n",
        "\n",
        "\n",
        "def probas_to_top_five(y_prob):\n",
        "    \"\"\"Convert the vector of probabilities assigned to each genre to a a list containing the five genres with max probability.\"\"\"\n",
        "    indices = np.argsort(-y_prob)[:5]\n",
        "    top_five_genre = []\n",
        "    for ind in indices:\n",
        "        y_one_hot = np.zeros(19, dtype=int)\n",
        "        y_one_hot[ind] = 1\n",
        "        top_five_genre.append(MULTILABEL_BINARIZER.inverse_transform(np.expand_dims(y_one_hot, axis=0))[0][0])\n",
        "    return top_five_genre\n",
        "\n",
        "\n",
        "def generate_lstm(input_length=MAX_VECTOR_LEN, dropout_rate=DROUPOUT_RATE, hidden_dim=HIDDEN_DIM):\n",
        "    \"\"\"Generate a lstm model.\"\"\"\n",
        "    model_lstm = Sequential()\n",
        "    model_lstm.add(Embedding(MAX_WORDS, EMB_DIM, input_length=input_length))\n",
        "    model_lstm.add(LSTM(hidden_dim,\n",
        "                        dropout=dropout_rate,\n",
        "                        return_sequences=False))\n",
        "    model_lstm.add(Dense(NUMBER_OF_GENRES, activation='softmax'))\n",
        "    optimizer = RMSprop(learning_rate=LEARNING_RATE)\n",
        "    model_lstm.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['categorical_accuracy'])\n",
        "    return model_lstm\n",
        "\n",
        "def generate_gru(input_length=MAX_VECTOR_LEN,dropout_rate=DROUPOUT_RATE,hidden_dim=HIDDEN_DIM):\n",
        "    model_gru = Sequential()\n",
        "    model_gru.add(Embedding(MAX_WORDS, EMB_DIM, input_length=input_length))\n",
        "    model_gru.add(GRU(hidden_dim,dropout=dropout_rate,return_sequences=False))\n",
        "    model_gru.add(Dense(NUMBER_OF_GENRES, activation = 'softmax'))\n",
        "    optimizer = Adam(learning_rate=LEARNING_RATE)\n",
        "    model_gru.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['categorical_accuracy'])\n",
        "    return model_gru"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Amf18EfrLpSC"
      },
      "source": [
        "Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-7XnnHoFmDJ",
        "outputId": "4984ac44-f28b-4928-9ccb-36eec8f9761a"
      },
      "source": [
        "df_movies = pd.read_csv('/content/gdrive/MyDrive/Radix data/train.csv')\n",
        "msk = np.random.rand(len(df_movies)) < 0.8\n",
        "df_movies_train = df_movies[msk]\n",
        "df_movies_test = df_movies[~msk]\n",
        "\n",
        "x_train = get_embedded_synopsis(df_movies_train)\n",
        "y_train = get_one_hot_genres(df_movies_train)\n",
        "model = generate_gru()\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          validation_split=0.1,\n",
        "          epochs=N_EPOCHS,\n",
        "          callbacks=[EARLY_STOPPING])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "206/206 [==============================] - 28s 100ms/step - loss: 2.5009 - categorical_accuracy: 0.2504 - val_loss: 2.2185 - val_categorical_accuracy: 0.2780\n",
            "Epoch 2/10\n",
            "206/206 [==============================] - 20s 97ms/step - loss: 2.0552 - categorical_accuracy: 0.3259 - val_loss: 1.9187 - val_categorical_accuracy: 0.4197\n",
            "Epoch 3/10\n",
            "206/206 [==============================] - 20s 96ms/step - loss: 1.6561 - categorical_accuracy: 0.4936 - val_loss: 1.8539 - val_categorical_accuracy: 0.4361\n",
            "Epoch 4/10\n",
            "206/206 [==============================] - 20s 97ms/step - loss: 1.3907 - categorical_accuracy: 0.5749 - val_loss: 1.8519 - val_categorical_accuracy: 0.4426\n",
            "Epoch 5/10\n",
            "206/206 [==============================] - 20s 97ms/step - loss: 1.2287 - categorical_accuracy: 0.6071 - val_loss: 1.9276 - val_categorical_accuracy: 0.4341\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9f9cbb4d10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlV-t0kRrC-j"
      },
      "source": [
        "with open('tokenizer.json') as f: \n",
        "    tokenizer = text.tokenizer_from_json(json.load(f))\n",
        "x_test = get_embedded_synopsis(df_movies_test,tokenizer=tokenizer)\n",
        "y_test = get_one_hot_genres(df_movies_test)\n",
        "prediction = model.predict(x_test)\n",
        "pred_to_five = [probas_to_top_five(y) for y in prediction]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIxXmJRpntfF",
        "outputId": "78a5bce5-5a20-4278-b4a5-e457f96b214e"
      },
      "source": [
        "def average_precision_score(one_hot_actual, predicted_proba, k=5):\n",
        "    predicted_list = probas_to_top_five(predicted_proba)\n",
        "    one_hot_actual = one_hot_actual>0\n",
        "    actual_list = list(MULTILABEL_BINARIZER.inverse_transform(np.expand_dims(one_hot_actual, axis=0))[0])\n",
        "    score = 0.0\n",
        "    num_hits = 0.0\n",
        "    for i,p in enumerate(predicted_list):\n",
        "        if p in actual_list and p not in predicted_list[:i]:\n",
        "            num_hits += 1.0\n",
        "            score += num_hits / (i + 1.0)\n",
        "    if not actual_list:\n",
        "        return 1.0\n",
        "    if min(len(actual_list), k) == 0:\n",
        "        return 0.0\n",
        "    else:\n",
        "        return score / min(len(actual_list), k)\n",
        "\n",
        "score = np.mean([average_precision_score(y_test[i], prediction[i]) for i in range(len(prediction))])\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6564793593814717\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCJzufuvZibw",
        "outputId": "d68973be-39de-444a-ef8f-80e78a45265b"
      },
      "source": [
        "def apk(actual, predicted, k=5):\n",
        "    score = 0.0\n",
        "    num_hits = 0.0\n",
        "    for i,p in enumerate(predicted):\n",
        "        if p in actual and p not in predicted[:i]:\n",
        "            num_hits += 1.0\n",
        "            score += num_hits / (i + 1.0)\n",
        "    if not actual:\n",
        "        return 1.0\n",
        "    if min(len(actual), k) == 0:\n",
        "        return 0.0\n",
        "    else:\n",
        "        return score / min(len(actual), k)\n",
        "        \n",
        "y_test_array = df_movies_test.genres.apply(lambda x: x.split(\" \")).to_numpy()\n",
        "score = np.mean([apk(y_test_array[i],pred_to_five[i]) for i in range(len(pred_to_five))])\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.38333737662298406\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_AKGmIOpzsv"
      },
      "source": [
        "df_movies = pd.read_csv('/content/gdrive/MyDrive/Radix data/test.csv')\n",
        "x_test = get_embedded_synopsis(df_movies)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5O6qTI6-iQRs"
      },
      "source": [
        "genre_mapping = {g:i for i,g in enumerate(set(x for g in df_movies['clean_genres'] for x in g))}\n",
        "to_int = [[genre_mapping[x] for x in y] for y in  ytrain.to_numpy()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVrm7bbofYQ8"
      },
      "source": [
        "def generate_gru(input_length=MAX_VECTOR_LEN,dropout_rate=DROUPOUT_RATE,hidden_dim=HIDDEN_DIM):\n",
        "    model_gru = Sequential()\n",
        "    model_gru.add(Embedding(MAX_WORDS, EMB_DIM , input_length=input_length))\n",
        "    model_gru.add(GRU(hidden_dim,dropout=dropout_rate, return_sequences=True))\n",
        "    model_gru.add(GRU(hidden_dim,dropout=dropout_rate,return_sequences=False))\n",
        "    model_gru.add(Dense(NUMBER_OF_GENRES, activation = 'softmax'))\n",
        "    optimizer = Adam(learning_rate=LEARNING_RATE)\n",
        "    model_gru.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['categorical_accuracy'])\n",
        "    return model_gru"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SFBLe5GpI18"
      },
      "source": [
        "DROUPOUT_RATE = 0.8\n",
        "HIDDEN_DIM = 128\n",
        "LEARNING_RATE = 0.002\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=1)\n",
        "model_gru = Sequential()\n",
        "model_gru.add(Embedding(max_words, emb_dim , input_length=xtrain_pad.shape[1]))\n",
        "model_gru.add(GRU(HIDDEN_DIM, return_sequences=True))\n",
        "model_gru.add(Dropout(DROUPOUT_RATE))\n",
        "model_gru.add(GRU(HIDDEN_DIM, return_sequences=False))\n",
        "model_gru.add(Dropout(DROUPOUT_RATE))\n",
        "model_gru.add(Dense(19, activation = 'softmax'))\n",
        "optimizer = Adam(learning_rate=LEARNING_RATE)\n",
        "model_gru.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['categorical_accuracy'])\n",
        "\n",
        "batch_size = 256\n",
        "epochs  = 10\n",
        "history_gru = model_gru.fit(xtrain_pad, ytrain_one_hot, validation_split=0.1, batch_size = batch_size, epochs = epochs,callbacks=[es,])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}